{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd37bb9b-b5a8-4754-881d-8ddac37d8872",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# GFL Prototype for P&ID Symbol Detection\n",
    "# Implementation of Kim et al.'s two-network strategy with GFL\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bbd62bc-c067-4469-ab49-2c3cb80f8e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import ipywidgets as widgets\n",
    "widgets.IntSlider()\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb7994-7670-4ce8-9fc6-2f3f8a2f38b7",
   "metadata": {},
   "source": [
    "# --- Data Loader Initialization ---\n",
    "# Insert this cell here to set up your dataset and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4017e37d-1f1d-4ece-a30a-d293bf1134a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path: notebooks/modeling/gfl_prototype.ipynb\n",
    "# Change: Insert after \"## Data Loader Integration\" markdown\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'/Users/mokshdutt/developer/P&ID/src/preprocessing')  # Adjust if your path differs\n",
    "\n",
    "from coco_dataloader import CocoDetectionDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "793c53c9-edc2-4d71-8a21-5c3abd82000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell right before DataLoader initialization\n",
    "def collate_padd(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    max_h = max(img.shape[-2] for img in images)\n",
    "    max_w = max(img.shape[-1] for img in images)\n",
    "    \n",
    "    padded_images = []\n",
    "    for img in images:\n",
    "        padding = (0, max_w - img.shape[-1], 0, max_h - img.shape[-2])\n",
    "        padded_images.append(torch.nn.functional.pad(img, padding))\n",
    "    \n",
    "    return torch.stack(padded_images), list(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c759c808-098a-43d3-91b2-8c6a027fbe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 500\n"
     ]
    }
   ],
   "source": [
    "# Path: notebooks/modeling/gfl_prototype.ipynb\n",
    "# Change: Insert after Data Loader import cell\n",
    "# Create transform pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Critical for converting PIL to tensor\n",
    "    # Add other transforms here if needed (resize, normalize, etc.)\n",
    "])\n",
    "\n",
    "dataset = CocoDetectionDataset(\n",
    "    root=r'/Users/mokshdutt/developer/P&ID/data/raw/paliwal_dataset/Images',\n",
    "    annFile=r'/Users/mokshdutt/developer/P&ID/data/processed/annotations/paliwal_coco.json',\n",
    "    transform= transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=1, shuffle=True, collate_fn=collate_padd, pin_memory=False, num_workers=0, persistent_workers=False)\n",
    "    \n",
    "\n",
    "print(f\"Total samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802fbe5-7678-4596-ba7a-f20a42091453",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 1. GFL Loss Functions Implementation\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60674690-4ec5-45a1-be8e-2ac9ad658328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityFocalLoss(nn.Module):\n",
    "    \"\"\"Quality Focal Loss implementation for GFL detector[4][9].\"\"\"\n",
    "    \n",
    "    def __init__(self, beta=2.0, reduction='mean'):\n",
    "        super(QualityFocalLoss, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: Predicted logits [N, num_classes]\n",
    "            targets: Target IoU-aware classification scores [N, num_classes]\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        modulating_factor = torch.pow(torch.abs(targets - probs), self.beta)\n",
    "        loss = modulating_factor * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "class DistributionFocalLoss(nn.Module):\n",
    "    \"\"\"Distribution Focal Loss for bounding box regression[4][9].\"\"\"\n",
    "    \n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(DistributionFocalLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: Predicted distribution [N, n+1] before softmax\n",
    "            target: Target distance values [N]\n",
    "        \"\"\"\n",
    "        dis_left = target.long()\n",
    "        dis_right = dis_left + 1\n",
    "        weight_left = dis_right.float() - target\n",
    "        weight_right = target - dis_left.float()\n",
    "        \n",
    "        loss = F.cross_entropy(pred, dis_left, reduction='none') * weight_left + \\\n",
    "               F.cross_entropy(pred, dis_right, reduction='none') * weight_right\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d80a8-2662-4d93-98df-6504d916e4f4",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 2. ResNet Backbone Implementation\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "095dfbf3-5fb9-4aae-8f6a-e958eb24964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)  # In-place activation\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "    \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)  # In-place operation\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        identity = self.shortcut(identity)  # Apply shortcut if exists\n",
    "        out += identity\n",
    "        return self.relu(out)  # In-place final activation\n",
    "\n",
    "class ResNetBackbone(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)  # ADD THIS LINE\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Feature pyramid levels\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "    \n",
    "    # ADD THIS METHOD IF MISSING\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract multi-scale features\n",
    "        c1 = self.relu(self.bn1(self.conv1(x)))  # Now uses self.relu\n",
    "        c1 = self.maxpool(c1)\n",
    "        \n",
    "        c2 = self.layer1(c1)  # 1/4 resolution\n",
    "        c3 = self.layer2(c2)  # 1/8 resolution\n",
    "        c4 = self.layer3(c3)  # 1/16 resolution\n",
    "        c5 = self.layer4(c4)  # 1/32 resolution\n",
    "        \n",
    "        return [c2, c3, c4, c5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab80be8-79a9-4dc3-8985-4084c56c41bf",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 3. GFL Detection Head Implementation\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "669fbb78-ad95-4d7b-ac0a-87fef275ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GFLHead(nn.Module):\n",
    "    \"\"\"GFL detection head with quality-aware classification and distribution regression[4][9].\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=256, num_classes=8, num_convs=4, reg_max=16):\n",
    "        super(GFLHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.reg_max = reg_max\n",
    "        \n",
    "        # Shared convolutions\n",
    "        self.cls_convs = nn.ModuleList()\n",
    "        self.reg_convs = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_convs):\n",
    "            chn = in_channels if i == 0 else in_channels\n",
    "            self.cls_convs.append(\n",
    "                nn.Conv2d(chn, in_channels, 3, stride=1, padding=1))\n",
    "            self.reg_convs.append(\n",
    "                nn.Conv2d(chn, in_channels, 3, stride=1, padding=1))\n",
    "        \n",
    "        # Output layers\n",
    "        self.gfl_cls = nn.Conv2d(in_channels, num_classes, 3, padding=1)\n",
    "        self.gfl_reg = nn.Conv2d(in_channels, 4 * (reg_max + 1), 3, padding=1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize layer weights[4].\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, feats):\n",
    "        \"\"\"Forward pass through GFL head.\"\"\"\n",
    "        cls_outputs = []\n",
    "        reg_outputs = []\n",
    "        \n",
    "        for feat in feats:\n",
    "            cls_feat = feat\n",
    "            reg_feat = feat\n",
    "            \n",
    "            # Classification branch\n",
    "            for cls_conv in self.cls_convs:\n",
    "                cls_feat = F.relu(cls_conv(cls_feat))\n",
    "            cls_score = self.gfl_cls(cls_feat)\n",
    "            \n",
    "            # Regression branch\n",
    "            for reg_conv in self.reg_convs:\n",
    "                reg_feat = F.relu(reg_conv(reg_feat))\n",
    "            bbox_pred = self.gfl_reg(reg_feat)\n",
    "            \n",
    "            cls_outputs.append(cls_score)\n",
    "            reg_outputs.append(bbox_pred)\n",
    "        \n",
    "        return cls_outputs, reg_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89fcce0-6c78-41c2-8cec-0a70d10df051",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 4. Complete GFL Detector\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f403281-1c29-45e0-a623-6176670239ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GFLDetector(nn.Module):\n",
    "    \"\"\"Complete GFL detector combining ResNet backbone and GFL head[4][11].\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=8, reg_max=16):\n",
    "        super(GFLDetector, self).__init__()\n",
    "        \n",
    "        # ResNet-50 backbone\n",
    "        self.backbone = ResNetBackbone(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "        \n",
    "        # Feature Pyramid Network (simplified)\n",
    "        self.fpn = nn.ModuleList([\n",
    "            nn.Conv2d(64, 256, 1),  # P2\n",
    "            nn.Conv2d(128, 256, 1),  # P3\n",
    "            nn.Conv2d(256, 256, 1),  # P4\n",
    "            nn.Conv2d(512, 256, 1),  # P5\n",
    "        ])\n",
    "        \n",
    "        # GFL detection head\n",
    "        self.head = GFLHead(256, num_classes, reg_max=reg_max)\n",
    "        \n",
    "        # Loss functions\n",
    "        self.qfl_loss = QualityFocalLoss()\n",
    "        self.dfl_loss = DistributionFocalLoss()\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        # Backbone feature extraction\n",
    "        backbone_feats = self.backbone(x)\n",
    "        \n",
    "        # FPN processing\n",
    "        fpn_feats = []\n",
    "        for i, feat in enumerate(backbone_feats):\n",
    "            fpn_feats.append(self.fpn[i](feat))\n",
    "        \n",
    "        # Detection head\n",
    "        cls_outputs, reg_outputs = self.head(fpn_feats)\n",
    "        \n",
    "        if self.training and targets is not None:\n",
    "            # Calculate losses during training\n",
    "            return self.calculate_losses(cls_outputs, reg_outputs, targets)\n",
    "        else:\n",
    "            # Return predictions during inference\n",
    "            return cls_outputs, reg_outputs\n",
    "    \n",
    "    def calculate_losses(self, cls_outputs, reg_outputs, targets):\n",
    "        \"\"\"Calculate GFL losses[4].\"\"\"\n",
    "        # This is a simplified loss calculation\n",
    "        # In practice, you'd need proper anchor generation and matching\n",
    "        total_loss = 0\n",
    "        \n",
    "        for cls_out, reg_out in zip(cls_outputs, reg_outputs):\n",
    "            # Dummy loss calculation - replace with proper implementation\n",
    "            cls_loss = self.qfl_loss(cls_out.flatten(2).permute(0, 2, 1).flatten(0, 1), \n",
    "                                   torch.zeros_like(cls_out.flatten(2).permute(0, 2, 1).flatten(0, 1)))\n",
    "            total_loss += cls_loss\n",
    "        \n",
    "        return {'total_loss': total_loss, 'cls_loss': cls_loss}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dd86de-52d4-4df6-b835-5bbd9a3d432f",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 5. Data Preprocessing for Two-Network Strategy\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea6ba143-598a-460b-851c-1aa8c6eceed7",
   "metadata": {},
   "source": [
    "class PIDSymbolDataset(Dataset):\n",
    "    \"\"\"Dataset class for P&ID symbols implementing Kim et al.'s strategy[11].\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, mode='small', transform=None, symbol_size_threshold=700):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.mode = mode  # 'small' or 'large'\n",
    "        self.transform = transform\n",
    "        self.threshold = symbol_size_threshold\n",
    "        self.samples = self._load_samples()\n",
    "    \n",
    "    def _load_samples(self):\n",
    "        \"\"\"Load and filter samples based on symbol size[11].\"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        # Load Paliwal dataset\n",
    "        paliwal_dir = self.data_dir / 'raw' / 'paliwal_dataset'\n",
    "        \n",
    "        for folder in paliwal_dir.iterdir():\n",
    "            if folder.is_dir() and folder.name.isdigit():\n",
    "                try:\n",
    "                    image_id = int(folder.name)\n",
    "                    image_path = paliwal_dir / 'Images' / f\"{image_id}.jpg\"\n",
    "                    \n",
    "                    if not image_path.exists():\n",
    "                        continue\n",
    "                        \n",
    "                    # Load symbol annotations\n",
    "                    symbols_file = folder / f\"{image_id}_symbols.npy\"\n",
    "                    if symbols_file.exists():\n",
    "                        symbols_data = np.load(symbols_file, allow_pickle=True).item()\n",
    "                        \n",
    "                        # Filter by symbol size\n",
    "                        filtered_symbols = self._filter_by_size(symbols_data)\n",
    "                        if filtered_symbols:\n",
    "                            samples.append({\n",
    "                                'image_path': image_path,\n",
    "                                'symbols_data': symbols_data,\n",
    "                                'filtered_indices': filtered_symbols\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {folder}: {e}\")\n",
    "        \n",
    "        print(f\"Loaded {len(samples)} samples for {self.mode} network\")\n",
    "        return samples\n",
    "    \n",
    "    def _filter_by_size(self, symbols_data):\n",
    "        \"\"\"Filter symbols based on diagonal length[11].\"\"\"\n",
    "        filtered = []\n",
    "        \n",
    "        if 'bounding_box' in symbols_data:\n",
    "            for i, bbox in enumerate(symbols_data['bounding_box']):\n",
    "                if len(bbox) >= 4:\n",
    "                    x1, y1, x2, y2 = bbox[:4]\n",
    "                    diagonal = np.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "                    \n",
    "                    if self.mode == 'small' and diagonal <= self.threshold:\n",
    "                        filtered.append(i)\n",
    "                    elif self.mode == 'large' and diagonal > self.threshold:\n",
    "                        filtered.append(i)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load actual image\n",
    "        image = Image.open(sample['image_path']).convert(\"RGB\")\n",
    "        \n",
    "        # Get filtered annotations\n",
    "        symbols = sample['symbols_data']\n",
    "        filtered = sample['filtered_indices']\n",
    "        \n",
    "        # Extract bounding boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in filtered:\n",
    "            if i < len(symbols['bounding_box']) and i < len(symbols['class_ids']):\n",
    "                x1, y1, x2, y2 = symbols['bounding_box'][i][:4]\n",
    "                class_id = symbols['class_ids'][i]\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "                labels.append(class_id)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels\n",
    "        }\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "606193d0-758c-4109-8e11-2b172321d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIDSymbolDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode='small', transform=None, symbol_size_threshold=700):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.mode = mode\n",
    "        self.threshold = symbol_size_threshold\n",
    "        # Compose transforms with ToTensor first\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            *([transform] if transform else [])\n",
    "        ])\n",
    "        self.samples = self._load_samples()\n",
    "\n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        paliwal_dir = self.data_dir / 'raw' / 'paliwal_dataset'\n",
    "        for folder in paliwal_dir.iterdir():\n",
    "            if folder.is_dir():\n",
    "                try:\n",
    "                    symbols_file = folder / 'symbols.npy'\n",
    "                    if symbols_file.exists():\n",
    "                        symbols_data = np.load(symbols_file, allow_pickle=True).item()\n",
    "                        filtered_symbols = self._filter_by_size(symbols_data)\n",
    "                        if filtered_symbols:\n",
    "                            samples.append({\n",
    "                                'folder': folder,\n",
    "                                'symbols': symbols_data,\n",
    "                                'filtered_indices': filtered_symbols\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {folder}: {e}\")\n",
    "        print(f\"Loaded {len(samples)} samples for {self.mode} network\")\n",
    "        return samples\n",
    "\n",
    "    def _filter_by_size(self, symbols_data):\n",
    "        filtered = []\n",
    "        if 'bounding_box' in symbols_data:\n",
    "            for i, bbox in enumerate(symbols_data['bounding_box']):\n",
    "                if len(bbox) >= 4:\n",
    "                    x1, y1, x2, y2 = bbox[:4]\n",
    "                    diagonal = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "                    if self.mode == 'small' and diagonal <= self.threshold:\n",
    "                        filtered.append(i)\n",
    "                    elif self.mode == 'large' and diagonal > self.threshold:\n",
    "                        filtered.append(i)\n",
    "        return filtered\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image_path = sample['folder'] / 'image.jpg'  # Adjust if your image filenames differ\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Now a tensor\n",
    "        symbols = sample['symbols']\n",
    "        filtered = sample['filtered_indices']\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in filtered:\n",
    "            if i < len(symbols['bounding_box']) and i < len(symbols['class_ids']):\n",
    "                x1, y1, x2, y2 = symbols['bounding_box'][i][:4]\n",
    "                class_id = symbols['class_ids'][i]\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "                labels.append(class_id)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.long)\n",
    "        target = {'boxes': boxes, 'labels': labels}\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd97c0-423a-4239-a235-8f5857c40552",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 6. Adaptive NMS Implementation\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f23ead9-b0fe-4b00-b59e-35199f441261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_nms(boxes, scores, labels, iou_thresholds, score_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Adaptive Non-Maximum Suppression with class-specific IoU thresholds[24][25].\n",
    "    \n",
    "    Args:\n",
    "        boxes: Tensor [N, 4] in (x1, y1, x2, y2) format\n",
    "        scores: Tensor [N] detection scores\n",
    "        labels: Tensor [N] class labels\n",
    "        iou_thresholds: Dict mapping class_id to IoU threshold\n",
    "        score_threshold: Minimum score threshold\n",
    "    \"\"\"\n",
    "    # Filter by score threshold\n",
    "    valid_mask = scores > score_threshold\n",
    "    boxes = boxes[valid_mask]\n",
    "    scores = scores[valid_mask]\n",
    "    labels = labels[valid_mask]\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        return torch.empty(0, dtype=torch.long)\n",
    "    \n",
    "    # Sort by scores in descending order\n",
    "    sorted_indices = torch.argsort(scores, descending=True)\n",
    "    boxes = boxes[sorted_indices]\n",
    "    scores = scores[sorted_indices]\n",
    "    labels = labels[sorted_indices]\n",
    "    \n",
    "    keep = []\n",
    "    suppressed = torch.zeros(len(boxes), dtype=torch.bool)\n",
    "    \n",
    "    for i in range(len(boxes)):\n",
    "        if suppressed[i]:\n",
    "            continue\n",
    "            \n",
    "        keep.append(sorted_indices[i])\n",
    "        current_box = boxes[i:i+1]\n",
    "        current_label = labels[i]\n",
    "        \n",
    "        # Get class-specific IoU threshold\n",
    "        iou_threshold = iou_thresholds.get(current_label.item(), 0.5)\n",
    "        \n",
    "        # Calculate IoU with remaining boxes of the same class\n",
    "        remaining_boxes = boxes[i+1:]\n",
    "        remaining_labels = labels[i+1:]\n",
    "        same_class_mask = remaining_labels == current_label\n",
    "        \n",
    "        if same_class_mask.any():\n",
    "            same_class_boxes = remaining_boxes[same_class_mask]\n",
    "            ious = calculate_iou(current_box, same_class_boxes)\n",
    "            \n",
    "            # Suppress boxes with high IoU\n",
    "            suppress_mask = ious > iou_threshold\n",
    "            \n",
    "            # Map back to global indices\n",
    "            global_suppress_indices = torch.where(same_class_mask)[0] + i + 1\n",
    "            suppressed[global_suppress_indices[suppress_mask]] = True\n",
    "    \n",
    "    return torch.tensor(keep, dtype=torch.long)\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate IoU between box1 and box2[29].\"\"\"\n",
    "    # box1: [1, 4], box2: [N, 4]\n",
    "    \n",
    "    # Calculate intersection\n",
    "    x1_max = torch.max(box1[:, 0], box2[:, 0])\n",
    "    y1_max = torch.max(box1[:, 1], box2[:, 1])\n",
    "    x2_min = torch.min(box1[:, 2], box2[:, 2])\n",
    "    y2_min = torch.min(box1[:, 3], box2[:, 3])\n",
    "    \n",
    "    intersection = torch.clamp(x2_min - x1_max, min=0) * torch.clamp(y2_min - y1_max, min=0)\n",
    "    \n",
    "    # Calculate areas\n",
    "    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "    \n",
    "    # Calculate union\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / (union + 1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea4001f-3170-4617-b8d5-dc1407ee059c",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 7. Training Setup and Utilities\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faf83277-b2db-4689-803c-441175701b2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 8.15 GB, other allocations: 672.00 KB, max allowed: 1.07 GB). Tried to allocate 36.75 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m     large_net = GFLDetector(num_classes=num_classes, reg_max=reg_max).to(device)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m small_net, large_net\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m small_net, large_net = \u001b[43mcreate_gfl_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModels initialized on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcreate_gfl_models\u001b[39m\u001b[34m(num_classes, reg_max, device)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mCreate both small and large symbol detection networks.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mReturns models moved to the specified device.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Small symbol network (for patches)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m small_net = \u001b[43mGFLDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_max\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreg_max\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Large symbol network (for full images)\u001b[39;00m\n\u001b[32m     10\u001b[39m large_net = GFLDetector(num_classes=num_classes, reg_max=reg_max).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/PID/lib/python3.11/site-packages/torch/nn/modules/module.py:1340\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1338\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/PID/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/PID/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/PID/lib/python3.11/site-packages/torch/nn/modules/module.py:927\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    925\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    930\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/PID/lib/python3.11/site-packages/torch/nn/modules/module.py:1326\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1320\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1321\u001b[39m             device,\n\u001b[32m   1322\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1323\u001b[39m             non_blocking,\n\u001b[32m   1324\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1325\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 8.15 GB, other allocations: 672.00 KB, max allowed: 1.07 GB). Tried to allocate 36.75 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "def create_gfl_models(num_classes=8, reg_max=16, device='mps'):\n",
    "    \"\"\"\n",
    "    Create both small and large symbol detection networks.\n",
    "    Returns models moved to the specified device.\n",
    "    \"\"\"\n",
    "    # Small symbol network (for patches)\n",
    "    small_net = GFLDetector(num_classes=num_classes, reg_max=reg_max).to(device)\n",
    "\n",
    "    # Large symbol network (for full images)\n",
    "    large_net = GFLDetector(num_classes=num_classes, reg_max=reg_max).to(device)\n",
    "\n",
    "    return small_net, large_net\n",
    "    \n",
    "small_net, large_net = create_gfl_models(num_classes=8, device=device)\n",
    "print(f\"Models initialized on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3394b7-afcc-4ff4-a427-df477683be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(model, learning_rate=1e-4):\n",
    "    \"\"\"\n",
    "    Setup AdamW optimizer and MultiStepLR scheduler for training.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8, 11], gamma=0.1)\n",
    "    return optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "618faf31-a4bd-48c9-a7c8-a3855fbaeb61",
   "metadata": {},
   "source": [
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch on the provided dataloader.\n",
    "    Handles batch device placement and loss accumulation.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
    "        # Move images and targets to device\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, targets)\n",
    "\n",
    "        # Assume model returns a dict with 'total_loss'\n",
    "        loss = outputs['total_loss']\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch completed. Average loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5bf4b72-2459-4ca2-ab2b-1c3a03cfb402",
   "metadata": {},
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, \n",
    "                grad_clip=None, grad_accum=1, \n",
    "                scaler=None, scheduler=None):\n",
    "    \"\"\"\n",
    "    Enhanced training loop with best practices for object detection.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    accumulated_loss = 0\n",
    "    optimizer.zero_grad()  # Initialize gradients\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    from tqdm.auto import tqdm\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training\", leave=False)\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "        # Device transfer with MPS compatibility\n",
    "        images = [img.to(device, non_blocking=device.type != 'mps') for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Mixed precision context (if available)\n",
    "        with torch.autocast(device_type='mps' if device.type == 'mps' else 'cuda', \n",
    "                          enabled=scaler is not None):\n",
    "            outputs = model(images, targets)\n",
    "            loss = outputs['total_loss'] / grad_accum  # Scale loss for accumulation\n",
    "\n",
    "        # Backward pass with scaler if using mixed precision\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        accumulated_loss += loss.item()\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (batch_idx + 1) % grad_accum == 0:\n",
    "            # Gradient clipping\n",
    "            if grad_clip:\n",
    "                if scaler:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            # Optimizer step\n",
    "            if scaler:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(loss=accumulated_loss)\n",
    "            total_loss += accumulated_loss\n",
    "            accumulated_loss = 0\n",
    "\n",
    "        # Step scheduler if available (e.g., for per-batch scheduling)\n",
    "        if scheduler and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step()\n",
    "\n",
    "    # Handle remaining accumulated gradients\n",
    "    if accumulated_loss > 0:\n",
    "        if scaler:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += accumulated_loss\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1cd7f06-e984-4441-b618-e5fe479b0807",
   "metadata": {},
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, \n",
    "                grad_clip=None, grad_accum=1, \n",
    "                scaler=None, scheduler=None):\n",
    "    \"\"\"\n",
    "    Enhanced training loop with Apple Silicon MPS support and error fixes.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    accumulated_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Progress bar with automatic MPS/CUDA detection\n",
    "    from tqdm.auto import tqdm\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training ({device.type.upper()})\", leave=False)\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "        # Convert to tensors and move to device (with MPS compatibility)\n",
    "        images = [img.to(device, non_blocking=(device.type == 'cuda')) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Mixed precision context (MPS has limited support)\n",
    "        with torch.autocast(device_type=device.type, enabled=scaler is not None):\n",
    "            outputs = model(images, targets)\n",
    "            loss = outputs['total_loss'] / grad_accum  # Scale for accumulation\n",
    "\n",
    "        # Backward pass\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        accumulated_loss += loss.item() * grad_accum  # Scale back for logging\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (batch_idx + 1) % grad_accum == 0:\n",
    "            # Gradient clipping\n",
    "            if grad_clip:\n",
    "                if scaler:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            # Optimizer step\n",
    "            if scaler:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update progress\n",
    "            progress_bar.set_postfix(loss=accumulated_loss)\n",
    "            total_loss += accumulated_loss\n",
    "            accumulated_loss = 0\n",
    "\n",
    "        # Scheduler step (for batch-based schedulers)\n",
    "        if scheduler and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step()\n",
    "\n",
    "    # Final gradient cleanup\n",
    "    if accumulated_loss > 0:\n",
    "        if scaler:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += accumulated_loss\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1233a488-e213-4551-87c6-a8506f585343",
   "metadata": {},
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, \n",
    "                grad_clip=None, grad_accum=1, \n",
    "                scaler=None, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    accumulated_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training ({device.type.upper()})\", leave=False)\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "        # Images are now tensors; safe to move to device\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.autocast(device_type=device.type, enabled=scaler is not None):\n",
    "            outputs = model(images, targets)\n",
    "            loss = outputs['total_loss'] / grad_accum\n",
    "\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        accumulated_loss += loss.item() * grad_accum\n",
    "\n",
    "        if (batch_idx + 1) % grad_accum == 0:\n",
    "            if grad_clip:\n",
    "                if scaler:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            if scaler:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.set_postfix(loss=accumulated_loss)\n",
    "            total_loss += accumulated_loss\n",
    "            accumulated_loss = 0\n",
    "\n",
    "        if scheduler and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step()\n",
    "\n",
    "    if accumulated_loss > 0:\n",
    "        if scaler:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += accumulated_loss\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e2aff-a443-4a16-985c-e56af28c5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, grad_clip=None, grad_accum=1, scaler=None, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    accumulated_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    from tqdm.auto import tqdm\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training ({device.type.upper()})\", leave=False)\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "        # === MEMORY FIX 1: Clear cache BEFORE processing ===\n",
    "        if device.type == 'mps':\n",
    "            torch.mps.empty_cache()\n",
    "        elif device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        images = images.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.autocast(device_type=device.type, enabled=scaler is not None):\n",
    "            outputs = model(images, targets)\n",
    "            loss = outputs['total_loss'] / grad_accum\n",
    "\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        # === MEMORY FIX 2: Synchronize AFTER backward ===\n",
    "        if device.type == 'mps':\n",
    "            torch.mps.synchronize()\n",
    "\n",
    "        accumulated_loss += loss.item() * grad_accum\n",
    "\n",
    "        if (batch_idx + 1) % grad_accum == 0:\n",
    "            if grad_clip:\n",
    "                if scaler:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            if scaler:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.set_postfix(loss=accumulated_loss)\n",
    "            total_loss += accumulated_loss\n",
    "            accumulated_loss = 0\n",
    "            \n",
    "            # === MEMORY FIX 3: Clear cache AFTER optimizer step ===\n",
    "            if device.type == 'mps':\n",
    "                torch.mps.empty_cache()\n",
    "\n",
    "    if accumulated_loss > 0:\n",
    "        if scaler:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += accumulated_loss\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e874e8-cbf9-41b9-9675-b77ae09fb63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Train GFL Model on Real Data ===\n",
    "num_epochs = 1  # Adjust as needed\n",
    "\n",
    "# Initialize models on the correct device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "small_net, large_net = create_gfl_models(num_classes=8, device=device)\n",
    "\n",
    "# Use small_net or large_net\n",
    "model = small_net\n",
    "\n",
    "# Setup training\n",
    "optimizer, scheduler = setup_training(model, learning_rate=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_epoch(model, dataloader, optimizer, device)\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1} complete. Avg loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e445c9-0c81-4774-b347-3d9305035256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, grad_clip=None, grad_accum=1, scaler=None, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    accumulated_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    from tqdm.auto import tqdm\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training ({device.type.upper()})\", leave=False)\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "        # Images are already padded and stacked by collate_fn\n",
    "        images = images.to(device)  # Direct tensor transfer\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Mixed precision context\n",
    "        with torch.autocast(device_type=device.type, enabled=scaler is not None):\n",
    "            outputs = model(images, targets)\n",
    "            loss = outputs['total_loss'] / grad_accum\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        accumulated_loss += loss.item() * grad_accum\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (batch_idx + 1) % grad_accum == 0:\n",
    "            if grad_clip:\n",
    "                if scaler:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            if scaler:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.set_postfix(loss=accumulated_loss)\n",
    "            total_loss += accumulated_loss\n",
    "            accumulated_loss = 0\n",
    "\n",
    "        # Scheduler step\n",
    "        if scheduler and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step()\n",
    "\n",
    "    # Handle remaining gradients\n",
    "    if accumulated_loss > 0:\n",
    "        if scaler:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += accumulated_loss\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58730ec-9e3b-42c7-b2ca-4ff2e64a8d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            # Verify tensor type\n",
    "            if not isinstance(images[0], torch.Tensor):\n",
    "                raise TypeError(\"Images must be tensors. Check dataset transforms.\")\n",
    "                \n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            outputs = model(images, targets)\n",
    "            loss = outputs['total_loss']\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Validation loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624d69c-74ab-4ebf-8d7e-17007e995167",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_epoch(model, dataloader, device)  # Use a separate val_dataloader if you split your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19e3f0-333c-42c8-b849-d1ce16b6d447",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 8. Prototype Testing and Visualization\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb2aa7-b6f0-4a74-9297-3b4d97dd5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gfl_prototype():\n",
    "    \"\"\"Test GFL implementation with MPS/device-aware dummy data.\"\"\"\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create models and move to device\n",
    "    small_net, large_net = create_gfl_models(num_classes=8, device=device)\n",
    "    \n",
    "    print(f\"Small network parameters: {sum(p.numel() for p in small_net.parameters()):,}\")\n",
    "    print(f\"Large network parameters: {sum(p.numel() for p in large_net.parameters()):,}\")\n",
    "    \n",
    "    # Create dummy input ON DEVICE\n",
    "    dummy_input = torch.randn(2, 3, 512, 512).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        small_net.eval()\n",
    "        cls_outputs, reg_outputs = small_net(dummy_input)\n",
    "        print(f\"Number of feature levels: {len(cls_outputs)}\")\n",
    "        for i, (cls_out, reg_out) in enumerate(zip(cls_outputs, reg_outputs)):\n",
    "            print(f\"Level {i}: cls_shape={cls_out.shape}, reg_shape={reg_out.shape}\")\n",
    "\n",
    "    # Test loss functions with device-aware tensors\n",
    "    qfl = QualityFocalLoss().to(device)\n",
    "    dfl = DistributionFocalLoss().to(device)\n",
    "    \n",
    "    dummy_cls_pred = torch.randn(100, 8).to(device)\n",
    "    dummy_cls_target = torch.rand(100, 8).to(device)\n",
    "    dummy_reg_pred = torch.randn(100, 17).to(device)\n",
    "    dummy_reg_target = torch.rand(100).to(device) * 16\n",
    "    \n",
    "    qfl_loss = qfl(dummy_cls_pred, dummy_cls_target)\n",
    "    dfl_loss = dfl(dummy_reg_pred, dummy_reg_target)\n",
    "    \n",
    "    print(f\"QFL Loss: {qfl_loss.item():.4f}\")\n",
    "    print(f\"DFL Loss: {dfl_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29762a7-fcd7-47a3-b010-70968b20eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_architecture():\n",
    "    \"\"\"Visualize the GFL architecture.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GFL DETECTOR ARCHITECTURE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "    Input Image\n",
    "         ↓\n",
    "    ResNet Backbone (Feature Extraction)\n",
    "         ↓\n",
    "    Feature Pyramid Network\n",
    "         ↓\n",
    "    GFL Detection Head\n",
    "    ├── Classification Branch (QFL)\n",
    "    └── Regression Branch (DFL)\n",
    "         ↓\n",
    "    Post-processing (Adaptive NMS)\n",
    "         ↓\n",
    "    Final Detections\n",
    "    \"\"\")\n",
    "    print(\"\\nTwo-Network Strategy:\")\n",
    "    print(\"├── Small Symbol Network: Patch-based detection (symbols < 700px)\")\n",
    "    print(\"└── Large Symbol Network: Full-image detection (symbols > 700px)\")\n",
    "\n",
    "# Run sanity checks and visualization\n",
    "test_gfl_prototype()\n",
    "visualize_architecture()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d971687-b752-4c56-80fb-32bf038a4dbb",
   "metadata": {},
   "source": [
    "def plot_image_with_boxes(img, boxes, labels=None, class_names=None):\n",
    "    \"\"\"Convert MPS tensors to CPU for visualization.\"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Convert to CPU if needed\n",
    "    if img.device.type == 'mps':\n",
    "        img = img.cpu()\n",
    "        boxes = boxes.cpu()\n",
    "        if labels is not None:\n",
    "            labels = labels.cpu()\n",
    "            \n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    for i, box in enumerate(boxes):\n",
    "        x, y, w, h = box.numpy()\n",
    "        rect = plt.Rectangle((x, y), w, h, fill=False, color='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        if labels is not None and class_names is not None:\n",
    "            label = class_names[labels[i]-1] if labels[i]-1 < len(class_names) else str(labels[i])\n",
    "            ax.text(x, y, label, color='yellow', fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb3f4e-0fbe-411c-bdd7-568229ad58dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_with_boxes(img, boxes, labels=None, class_names=None):\n",
    "    \"\"\"Visualize image with bounding boxes, handling both CPU/MPS tensors and numpy arrays.\"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Convert torch tensors to numpy arrays\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        # Handle device transfer\n",
    "        if img.device.type == 'mps':\n",
    "            img = img.cpu()\n",
    "        # Convert tensor to numpy and fix channel order\n",
    "        img = img.detach().permute(1, 2, 0).numpy()  # CHW -> HWC\n",
    "    \n",
    "    # Convert boxes if they're tensors\n",
    "    if isinstance(boxes, torch.Tensor):\n",
    "        if boxes.device.type == 'mps':\n",
    "            boxes = boxes.cpu()\n",
    "        boxes = boxes.detach().numpy()\n",
    "    \n",
    "    # Convert labels if they're tensors\n",
    "    if labels is not None and isinstance(labels, torch.Tensor):\n",
    "        if labels.device.type == 'mps':\n",
    "            labels = labels.cpu()\n",
    "        labels = labels.detach().numpy()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    for i, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = box  # Assume [x1, y1, x2, y2] format\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "        \n",
    "        rect = plt.Rectangle((x1, y1), w, h, \n",
    "                            fill=False, color='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        if labels is not None and class_names is not None:\n",
    "            label_idx = int(labels[i]) - 1  # Convert to 0-based index\n",
    "            label = class_names[label_idx] if 0 <= label_idx < len(class_names) else str(labels[i])\n",
    "            ax.text(x1, y1, label, color='white', fontsize=10, \n",
    "                    bbox=dict(facecolor='red', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
